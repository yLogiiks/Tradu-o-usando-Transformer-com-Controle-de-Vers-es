# Minha experi√™ncia com o tutorial de Transformer do TensorFlow

## O que entendi sobre o tutorial e sua import√¢ncia

O tutorial de Transformer do TensorFlow me mostrou, de forma pr√°tica, como funciona a arquitetura que hoje √© a base de modelos de linguagem modernos (como BERT e GPT). Eu percebi que a import√¢ncia dele n√£o est√° s√≥ em ensinar a programar um tradutor, mas sim em mostrar como o Transformer pensa, como ele organiza a informa√ß√£o e como isso o torna t√£o poderoso em tarefas de NLP.  

## Estrutura e O que entendi sobre cada etapa

1. **Prepara√ß√£o dos dados**  
   Percebi que o dataset n√£o pode ser usado ‚Äúcru‚Äù. Ele precisa passar por v√°rias etapas:  
   - **Tokeniza√ß√£o**: cada frase √© quebrada em tokens (palavras ou subpalavras).  
   - **Adi√ß√£o de tokens especiais**: √© necess√°rio adicionar marcadores como `<start>` e `<end>` para indicar onde a frase come√ßa e termina.  
   - **Padding**: frases mais curtas recebem preenchimento (zeros) para que todas fiquem do mesmo tamanho.  
   - **Truncamento**: frases muito longas s√£o cortadas para n√£o ultrapassar o limite definido.  
   Entendi que, sem esse preparo, o modelo n√£o teria como alinhar as entradas com as sa√≠das corretamente, e o treino n√£o funcionaria.

2. **Embeddings posicionais**  
   O embedding posicional √© o que d√° ao modelo uma no√ß√£o da posi√ß√£o de cada palavra. O que me chamou aten√ß√£o foi o uso de senos e cossenos para gerar padr√µes num√©ricos diferentes em cada posi√ß√£o ‚Äî uma forma matem√°tica bem elegante de dar ‚Äúordem‚Äù √†s palavras. Eu percebi que isso evita confundir frases como:  
   - ‚Äúo cachorro mordeu o menino‚Äù  
   - ‚Äúo menino mordeu o cachorro‚Äù

3. **Mecanismo de aten√ß√£o (self-attention)**  
   Aqui entendi que o modelo compara cada palavra com todas as outras da frase para decidir quais s√£o mais relevantes. O exemplo que ficou mais claro para mim foi quando o sujeito e o verbo est√£o distantes: o modelo consegue ligar ‚Äúmenino‚Äù com ‚Äúmordeu‚Äù, mesmo que existam palavras no meio. Isso me mostrou como o Transformer √© capaz de capturar rela√ß√µes de longo alcance, algo que RNNs tradicionais tinham dificuldade.

4. **Constru√ß√£o do encoder e decoder**  
   O encoder gera uma representa√ß√£o rica da frase de entrada (em portugu√™s), enquanto o decoder vai ‚Äúconsultando‚Äù essa representa√ß√£o para gerar a frase de sa√≠da (em ingl√™s). Eu achei interessante como cada camada acrescenta mais abstra√ß√£o, quase como se o modelo fosse refinando o entendimento da frase a cada n√≠vel.

5. **Treinamento do modelo**  
   Aqui percebi que, logo nas primeiras √©pocas, o modelo j√° come√ßava a dar tradu√ß√µes rudimentares, e conforme o treino avan√ßava, as frases iam ficando mais coerentes. Mesmo sem GPU, consegui notar essa evolu√ß√£o, o que refor√ßou a ideia de que o Transformer √© eficiente em aprender padr√µes de linguagem.

6. **Gera√ß√£o de tradu√ß√µes**  
   Testar frases no modelo foi a parte mais gratificante. Notei que ele n√£o fazia tradu√ß√µes palavra a palavra, mas tentava captar o contexto. Por exemplo, em algumas frases, ele adaptava a ordem para ficar mais natural em ingl√™s, o que mostrou que realmente havia aprendizado sem√¢ntico.

7. **Exporta√ß√£o do modelo**  
   Eu percebi que esse passo √© o que torna o aprendizado realmente √∫til: salvar o modelo significa poder reaproveit√°-lo em projetos futuros, sem ter que treinar tudo de novo. Para mim, foi uma das etapas que mais conectou teoria com pr√°tica.


## Objetivo e Dataset usado

O objetivo do tutorial foi implementar, do zero, um modelo Transformer para tradu√ß√£o autom√°tica **Portugu√™s ‚Üí Ingl√™s**.  
O dataset utilizado foi o **TED Talks Open Translation Project (TED HRLR)**, que cont√©m pares de frases em portugu√™s e ingl√™s.

## Como rodar o projeto (CPU/GPU)

### Depend√™ncias
- Python 3.10+
- TensorFlow 2.x
- TensorFlow Text
- NumPy
- Matplotlib

### Comandos principais
```bash
# Instala√ß√£o de depend√™ncias
pip install tensorflow tensorflow-text matplotlib

# Rodar no CPU
python transformer_tutorial.py

# Rodar no GPU
CUDA_VISIBLE_DEVICES=0 python transformer_tutorial.py
````

### Resultados: CPU vs GPU

| Ambiente | Tempo por √©poca | Tempo total | Loss final |
| -------- | --------------- | ----------- | ---------- |
| **CPU**  | \~20 min        | \~2h+       | \~2.1      |
| **GPU**  | \~2 min         | \~12‚Äì15 min | \~2.1      |

**Observa√ß√£o:**
Os n√∫meros acima s√£o estimativas baseadas no comportamento tutorial. No meu caso, **n√£o consegui completar o treinamento inteiro em CPU** pela demora.

### Gr√°ficos e Visualiza√ß√µes

1. **Perda (loss) por √©poca**
   O gr√°fico mostra a curva de aprendizado diminuindo progressivamente com o avan√ßo das √©pocas.

### Pontos Positivos üëç

- **Entendimento profundo**: Cada etapa me fez refletir sobre como o Transformer funciona de forma integrada, n√£o apenas como blocos separados.
- **Exemplos pr√°ticos bem conectados**: Gostei de como cada trecho de c√≥digo vinha acompanhado de explica√ß√µes e era usado logo em seguida.
- **Integra√ß√£o com Keras**: A clareza na defini√ß√£o das camadas deixou o modelo mais acess√≠vel.
- **Resultados animadores**: Foi muito bom ver que mesmo um modelo relativamente simples j√° consegue gerar tradu√ß√µes razo√°veis.


### Pontos Negativos üëé

- **Curva de aprendizado**: Algumas partes (principalmente aten√ß√£o multi-cabe√ßas) exigiram pesquisa extra para eu realmente entender.
- **Notebook longo**: Em alguns trechos, senti que estava apenas seguindo o fluxo sem absorver tudo.
- **Treinamento pesado**: Sem GPU, o tempo de treino foi um obst√°culo que atrapalhou a experi√™ncia.
- **Aplica√ß√£o limitada**: O foco foi apenas em tradu√ß√£o, e eu gostaria de ver outros exemplos de aplica√ß√£o do mesmo modelo.

### Percep√ß√µes pessoais

- **O que foi f√°cil**: entender os conceitos b√°sicos de embeddings, encoder/decoder e testar tradu√ß√µes.
- **O que foi dif√≠cil**: compreender em detalhes a aten√ß√£o multi-cabe√ßas e lidar com a demora do treino.
- **Trade-offs**: o Transformer √© extremamente poderoso, mas exige muito recurso computacional.
- **Gargalo principal**: treinamento em CPU. √â invi√°vel completar o processo sem GPU.

### Pr√≥ximos passos

* Testar tokenizadores melhores (ex: SentencePiece, BPE).
* Explorar **KerasNLP**, que j√° tem blocos prontos para Transformers.
* Usar **mixed precision training** para acelerar no GPU.
* Aplicar o mesmo modelo em outras tarefas de NLP (resumo, classifica√ß√£o de texto).

### Conclus√£o

No geral, gostei bastante desse tutorial sobre NLP. Ele me deu n√£o apenas pr√°tica com TensorFlow, mas tamb√©m uma compreens√£o real do porqu√™ os Transformers mudaram o campo de processamento de linguagem natural.

**Observa√ß√£o:**
Apesar de toda a experi√™ncia, √© praticamente imposs√≠vel rodar o notebook inteiro em CPU pela demora que ele demanda. Uma GPU √© essencial para completar o treino.
